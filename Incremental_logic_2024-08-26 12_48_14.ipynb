{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2ee4ed-cc18-4231-9fa6-f84cce31ce89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Incremental_Load_Example\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ca040f-9a62-4b7e-ba8a-dcb3fbfbb842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the initial data\n",
    "initial_data = [\n",
    "    (1, \"Alice\", \"2024-08-01 10:00:00\"),\n",
    "    (2, \"Bob\", \"2024-08-02 11:30:00\"),\n",
    "    (3, \"Charlie\", \"2024-08-03 14:15:00\"),\n",
    "    (4, \"David\", \"2024-08-04 09:45:00\"),\n",
    "    (5, \"Eva\", \"2024-08-05 13:00:00\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"name\", \"last_modified\"]\n",
    "df_initial = spark.createDataFrame(initial_data, columns)\n",
    "\n",
    "# Write the initial data to a Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/initial_load\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5713aa89-09f9-4f16-a729-acfb0a224b21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_initial = df_initial.withColumn(\"last_modified\", col(\"last_modified\").cast(\"timestamp\"))\n",
    "\n",
    "# Get the watermark value (latest timestamp)\n",
    "watermark_df = df_initial.selectExpr(\"max(last_modified) as max_last_modified\")\n",
    "\n",
    "# Write the watermark to another Delta table\n",
    "watermark_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e9de17-4c2d-4be8-9b84-0f63eb6b9edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the incremental data\n",
    "incremental_data = [\n",
    "    (2, \"Bob Smith\", \"2024-08-06 11:30:00\"),  # Updated record for id=2\n",
    "    (5, \"Eva\", \"2024-08-07 13:00:00\"),        # Updated record for id=5\n",
    "    (6, \"Frank\", \"2024-08-07 15:00:00\"),      # New record\n",
    "    (7, \"Grace\", \"2024-08-07 16:00:00\")       # New record\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_incremental = spark.createDataFrame(incremental_data, columns)\n",
    "\n",
    "df_incremental = df_incremental.withColumn(\"last_modified\", col(\"last_modified\").cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4165d9e-20f9-48c2-9b18-14a2452e683e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the last watermark\n",
    "last_load_timestamp = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/watermark_table\").selectExpr(\"max(max_last_modified)\").collect()[0][0]\n",
    "\n",
    "# Filter the incremental data based on the last load timestamp\n",
    "new_changes_df = df_incremental.filter(f\"last_modified > '{last_load_timestamp}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d242ce-de86-499a-bbfa-e34beb9439f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the existing Delta table\n",
    "deltaTable = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/initial_load\")\n",
    "\n",
    "# Merge new changes into the existing Delta table\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    new_changes_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde034a7-f6bb-48bd-83c7-b9daac0a3e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the watermark with the new last_modified\n",
    "new_last_modified = new_changes_df.selectExpr(\"max(last_modified) as max_last_modified\").collect()[0][0]\n",
    "\n",
    "# Save the new watermark\n",
    "spark.createDataFrame([(new_last_modified,)], [\"max_last_modified\"]).write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78c7d56-f1ab-44f0-b7dd-7ac0c68036f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Incremental_Load_Example\").getOrCreate()\n",
    "\n",
    "# Define the initial data\n",
    "initial_data = [\n",
    "    (1, \"Alice\", \"2024-08-01 10:00:00\"),\n",
    "    (2, \"Bob\", \"2024-08-02 11:30:00\"),\n",
    "    (3, \"Charlie\", \"2024-08-03 14:15:00\"),\n",
    "    (4, \"David\", \"2024-08-04 09:45:00\"),\n",
    "    (5, \"Eva\", \"2024-08-05 13:00:00\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"name\", \"last_modified\"]\n",
    "df_initial = spark.createDataFrame(initial_data, columns)\n",
    "\n",
    "# Write the initial data to a Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/initial_load\")\n",
    "\n",
    "df_initial = df_initial.withColumn(\"last_modified\", col(\"last_modified\").cast(\"timestamp\"))\n",
    "\n",
    "# Get the watermark value (latest timestamp)\n",
    "watermark_df = df_initial.selectExpr(\"max(last_modified) as max_last_modified\")\n",
    "\n",
    "# Write the watermark to another Delta table\n",
    "watermark_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n",
    "\n",
    "# Define the incremental data\n",
    "incremental_data = [\n",
    "    (2, \"Bob Smith\", \"2024-08-06 11:30:00\"),  # Updated record for id=2\n",
    "    (5, \"Eva\", \"2024-08-07 13:00:00\"),        # Updated record for id=5\n",
    "    (6, \"Frank\", \"2024-08-07 15:00:00\"),      # New record\n",
    "    (7, \"Grace\", \"2024-08-07 16:00:00\")       # New record\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_incremental = spark.createDataFrame(incremental_data, columns)\n",
    "\n",
    "df_incremental = df_incremental.withColumn(\"last_modified\", col(\"last_modified\").cast(\"timestamp\"))\n",
    "\n",
    "# Read the last watermark\n",
    "last_load_timestamp = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/watermark_table\").selectExpr(\"max(max_last_modified)\").collect()[0][0]\n",
    "\n",
    "# Filter the incremental data based on the last load timestamp\n",
    "new_changes_df = df_incremental.filter(f\"last_modified > '{last_load_timestamp}'\")\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the existing Delta table\n",
    "deltaTable = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/initial_load\")\n",
    "\n",
    "# Merge new changes into the existing Delta table\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    new_changes_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "\n",
    "# Update the watermark with the new last_modified\n",
    "new_last_modified = new_changes_df.selectExpr(\"max(last_modified) as max_last_modified\").collect()[0][0]\n",
    "\n",
    "# Save the new watermark\n",
    "spark.createDataFrame([(new_last_modified,)], [\"max_last_modified\"]).write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e02914-0463-4c35-8925-e83f173d166a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+-------------+\n|           Timestamp|  Price|Volume|   Market Cap|\n+--------------------+-------+------+-------------+\n|2013-11-30 00:00:...|1127.45|   0.0|13593664650.0|\n|2013-12-01 00:00:...|1033.39|   0.0|12464724345.0|\n|2013-12-02 00:00:...| 974.03|   0.0|11753157346.0|\n|2013-12-03 00:00:...|1078.64|   0.0|13020236474.0|\n|2013-12-04 00:00:...|1121.48|   0.0|13542487814.0|\n|2013-12-05 00:00:...| 989.04|   0.0|11947405392.0|\n|2013-12-06 00:00:...|1019.66|   0.0|12322234219.0|\n|2013-12-07 00:00:...| 811.98|   0.0| 9816777302.0|\n|2013-12-08 00:00:...| 743.11|   0.0| 8987358118.0|\n|2013-12-09 00:00:...| 897.89|   0.0|10863885372.0|\n|2013-12-10 00:00:...| 927.78|   0.0|11230173843.0|\n|2013-12-11 00:00:...| 887.08|   0.0|10741252534.0|\n|2013-12-12 00:00:...| 878.17|   0.0|10636987805.0|\n|2013-12-13 00:00:...| 897.27|   0.0|10872198158.0|\n|2013-12-14 00:00:...| 866.99|   0.0|10509219285.0|\n|2013-12-15 00:00:...| 854.62|   0.0|10363485334.0|\n|2013-12-16 00:00:...| 789.15|   0.0| 9573257565.0|\n|2013-12-17 00:00:...| 691.65|   0.0| 8393847109.0|\n|2013-12-18 00:00:...| 559.36|   0.0| 6791343584.0|\n|2013-12-19 00:00:...| 677.84|   0.0| 8233502182.0|\n+--------------------+-------+------+-------------+\nonly showing top 20 rows\n\nOut[68]: ['Timestamp', 'Price', 'Volume', 'Market Cap']"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Inc Load\").getOrCreate()\n",
    "\n",
    "file_path = \"/FileStore/tables/Bitcoin_Merged_Data.csv\"\n",
    "df_initial = spark.read.option(\"header\", True).csv(file_path)\n",
    "df_initial.show()\n",
    "df_initial.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf957a57-f506-4f02-a7bd-06ea5b05607e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns to remove invalid characters\n",
    "df_initial = df_initial.withColumnRenamed(\"Market Cap\", \"Market_Cap\")\n",
    "\n",
    "# Now you can safely cast the Timestamp_ column to a timestamp type\n",
    "#df_initial = df_initial.withColumn(\"Timestamp_\", col(\"Timestamp_\").cast(\"timestamp\"))\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "#df_initial.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/load\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735d5a23-f8e4-4e74-8b8e-4b3a65616b22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the watermark (latest timestamp)\n",
    "watermark_df = df_initial.selectExpr(\"max(Timestamp) as max_last_modified\")\n",
    "\n",
    "# Write the watermark value to another Delta table\n",
    "watermark_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62652b51-670a-45ac-bc02-2aa654ca88e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the incremental data\n",
    "incremental_data = [\n",
    "    (\"2024-08-06 11:30:00\", 15000, 1.5, 300000),\n",
    "    (\"2024-08-07 13:00:00\", 20000, 2.0, 400000),\n",
    "    (\"2024-08-07 15:00:00\", 30000, 3.0, 600000),\n",
    "    (\"2024-08-07 16:00:00\", 25000, 2.5, 500000)\n",
    "]\n",
    "\n",
    "columns = [\"Timestamp\", \"Price\", \"Volume\", \"Market Cap\"]\n",
    "\n",
    "# Create a DataFrame with the incremental data\n",
    "df_incremental = spark.createDataFrame(incremental_data, columns)\n",
    "\n",
    "# Cast the Timestamp column to a timestamp type\n",
    "df_incremental = df_incremental.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8762b6-2cb5-44cd-bdbc-b9cfec461c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the last watermark\n",
    "last_load_timestamp = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/watermark_table\").selectExpr(\"max(max_last_modified)\").collect()[0][0]\n",
    "\n",
    "# Filter the incremental data based on the last load timestamp\n",
    "new_changes_df = df_incremental.filter(col(\"Timestamp\") > last_load_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d161e255-347a-4114-ada3-469294ed65ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns to remove spaces or special characters in both DataFrames\n",
    "df_initial = df_initial.withColumnRenamed(\"Market Cap\", \"Market_Cap\")\n",
    "new_changes_df = new_changes_df.withColumnRenamed(\"Market Cap\", \"Market_Cap\")\n",
    "\n",
    "# Load the Delta table\n",
    "deltaTable = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/load\")\n",
    "\n",
    "# Merge new changes into the existing Delta table\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    new_changes_df.alias(\"source\"),\n",
    "    \"target.Timestamp = source.Timestamp\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd3df16-afad-4b4c-9eca-ed261a9d5c16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the watermark with the new last_modified timestamp\n",
    "new_last_modified = new_changes_df.selectExpr(\"max(Timestamp) as max_last_modified\").collect()[0][0]\n",
    "spark.createDataFrame([(new_last_modified,)], [\"max_last_modified\"]).write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b8c4e1-a487-4b82-8ca8-c2770c9f68ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb70d181-1a39-4cf8-b10e-d2849a23248e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+-------------+\n|           Timestamp|  Price|Volume|   Market Cap|\n+--------------------+-------+------+-------------+\n|2013-11-30 00:00:...|1127.45|   0.0|13593664650.0|\n|2013-12-01 00:00:...|1033.39|   0.0|12464724345.0|\n|2013-12-02 00:00:...| 974.03|   0.0|11753157346.0|\n|2013-12-03 00:00:...|1078.64|   0.0|13020236474.0|\n|2013-12-04 00:00:...|1121.48|   0.0|13542487814.0|\n|2013-12-05 00:00:...| 989.04|   0.0|11947405392.0|\n|2013-12-06 00:00:...|1019.66|   0.0|12322234219.0|\n|2013-12-07 00:00:...| 811.98|   0.0| 9816777302.0|\n|2013-12-08 00:00:...| 743.11|   0.0| 8987358118.0|\n|2013-12-09 00:00:...| 897.89|   0.0|10863885372.0|\n|2013-12-10 00:00:...| 927.78|   0.0|11230173843.0|\n|2013-12-11 00:00:...| 887.08|   0.0|10741252534.0|\n|2013-12-12 00:00:...| 878.17|   0.0|10636987805.0|\n|2013-12-13 00:00:...| 897.27|   0.0|10872198158.0|\n|2013-12-14 00:00:...| 866.99|   0.0|10509219285.0|\n|2013-12-15 00:00:...| 854.62|   0.0|10363485334.0|\n|2013-12-16 00:00:...| 789.15|   0.0| 9573257565.0|\n|2013-12-17 00:00:...| 691.65|   0.0| 8393847109.0|\n|2013-12-18 00:00:...| 559.36|   0.0| 6791343584.0|\n|2013-12-19 00:00:...| 677.84|   0.0| 8233502182.0|\n+--------------------+-------+------+-------------+\nonly showing top 20 rows\n\n+-------------------+-------+------+-------------+\n|          Timestamp|  Price|Volume|   Market_Cap|\n+-------------------+-------+------+-------------+\n|2013-11-30 00:00:00|1127.45|   0.0|13593664650.0|\n|2013-12-01 00:00:00|1033.39|   0.0|12464724345.0|\n|2013-12-02 00:00:00| 974.03|   0.0|11753157346.0|\n|2013-12-03 00:00:00|1078.64|   0.0|13020236474.0|\n|2013-12-04 00:00:00|1121.48|   0.0|13542487814.0|\n|2013-12-05 00:00:00| 989.04|   0.0|11947405392.0|\n|2013-12-06 00:00:00|1019.66|   0.0|12322234219.0|\n|2013-12-07 00:00:00| 811.98|   0.0| 9816777302.0|\n|2013-12-08 00:00:00| 743.11|   0.0| 8987358118.0|\n|2013-12-09 00:00:00| 897.89|   0.0|10863885372.0|\n|2013-12-10 00:00:00| 927.78|   0.0|11230173843.0|\n|2013-12-11 00:00:00| 887.08|   0.0|10741252534.0|\n|2013-12-12 00:00:00| 878.17|   0.0|10636987805.0|\n|2013-12-13 00:00:00| 897.27|   0.0|10872198158.0|\n|2013-12-14 00:00:00| 866.99|   0.0|10509219285.0|\n|2013-12-15 00:00:00| 854.62|   0.0|10363485334.0|\n|2013-12-16 00:00:00| 789.15|   0.0| 9573257565.0|\n|2013-12-17 00:00:00| 691.65|   0.0| 8393847109.0|\n|2013-12-18 00:00:00| 559.36|   0.0| 6791343584.0|\n|2013-12-19 00:00:00| 677.84|   0.0| 8233502182.0|\n+-------------------+-------+------+-------------+\nonly showing top 20 rows\n\nroot\n |-- Timestamp: timestamp (nullable = true)\n |-- Price: string (nullable = true)\n |-- Volume: string (nullable = true)\n |-- Market_Cap: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "#Creating a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Inc Load\").getOrCreate()\n",
    "\n",
    "#Reading the CSV File into a DataFrame\n",
    "file_path = \"/FileStore/tables/Bitcoin_Merged_Data.csv\"\n",
    "df_initial = spark.read.option(\"header\", True).csv(file_path)\n",
    "df_initial.show()\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "df_initial = df_initial.withColumnRenamed(\"Market Cap\", \"Market_Cap\")\n",
    "\n",
    "# Now you can safely cast the Timestamp_ column to a timestamp type\n",
    "df_initial = df_initial.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/load\")\n",
    "\n",
    "# Calculate the watermark (latest timestamp)\n",
    "watermark_df = df_initial.selectExpr(\"max(Timestamp) as max_last_modified\")\n",
    "\n",
    "# Write the watermark value to another Delta table\n",
    "watermark_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n",
    "\n",
    "# Define the incremental data\n",
    "incremental_data = [\n",
    "    (\"2024-08-06 11:30:00\", 15000, 1.5, 300000),\n",
    "    (\"2024-08-07 13:00:00\", 20000, 2.0, 400000),\n",
    "    (\"2024-08-07 15:00:00\", 30000, 3.0, 600000),\n",
    "    (\"2024-08-07 16:00:00\", 25000, 2.5, 500000)\n",
    "]\n",
    "\n",
    "columns = [\"Timestamp\", \"Price\", \"Volume\", \"Market Cap\"]\n",
    "\n",
    "# Create a DataFrame with the incremental data\n",
    "df_incremental = spark.createDataFrame(incremental_data, columns)\n",
    "\n",
    "# Cast the Timestamp column to a timestamp type\n",
    "df_incremental = df_incremental.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Read the last watermark\n",
    "last_load_timestamp = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/watermark_table\").selectExpr(\"max(max_last_modified)\").collect()[0][0]\n",
    "\n",
    "# Filter the incremental data based on the last load timestamp\n",
    "new_changes_df = df_incremental.filter(col(\"Timestamp\") > last_load_timestamp)\n",
    "\n",
    "# Rename columns to remove spaces or special characters in both DataFrames\n",
    "df_initial = df_initial.withColumnRenamed(\"Market Cap\", \"Market_Cap\")\n",
    "new_changes_df = new_changes_df.withColumnRenamed(\"Market Cap\", \"Market_Cap\")\n",
    "\n",
    "# Load the Delta table\n",
    "deltaTable = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/load\")\n",
    "\n",
    "# Merge new changes into the existing Delta table\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    new_changes_df.alias(\"source\"),\n",
    "    \"target.Timestamp = source.Timestamp\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Update the watermark with the new last_modified timestamp\n",
    "new_last_modified = new_changes_df.selectExpr(\"max(Timestamp) as max_last_modified\").collect()[0][0]\n",
    "spark.createDataFrame([(new_last_modified,)], [\"max_last_modified\"]).write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/watermark_table\")\n",
    "\n",
    "deltaTable.toDF().show()\n",
    "\n",
    "deltaTable.toDF().printSchema()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Incremental_logic_2024-08-26 12:48:14",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

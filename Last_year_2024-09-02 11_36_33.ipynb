{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746b93cf-570b-4236-a83b-c1a94b117fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# /FileStore/tables/Sales_SalesOrderDetail-2.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a55bff-ce23-400d-8e37-514c2f38594a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+------------+\n|SalesOrderID|SalesOrderDetailID|CarrierTrackingNumber|OrderQty|ProductID|SpecialOfferID|UnitPrice|UnitPriceDiscount|LineTotal|             rowguid|ModifiedDate|\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+------------+\n|       43659|                 1|         4911-403C-98|       1|      776|             1|  2024.99|             0.00|  2024.99|B207C96D-D9E6-402...|  2011-05-31|\n|       43659|                 2|         4911-403C-98|       3|      777|             1|  2024.99|             0.00|  6074.98|7ABB600D-1E77-41B...|  2011-05-31|\n|       43659|                 3|         4911-403C-98|       1|      778|             1|  2024.99|             0.00|  2024.99|475CF8C6-49F6-486...|  2011-05-31|\n|       43659|                 4|         4911-403C-98|       1|      771|             1|  2039.99|             0.00|  2039.99|04C4DE91-5815-45D...|  2011-05-31|\n|       43659|                 5|         4911-403C-98|       1|      772|             1|  2039.99|             0.00|  2039.99|5A74C7D2-E641-438...|  2011-05-31|\n|       43659|                 6|         4911-403C-98|       2|      773|             1|  2039.99|             0.00|  4079.99|CE472532-A4C0-45B...|  2011-05-31|\n|       43659|                 7|         4911-403C-98|       1|      774|             1|  2039.99|             0.00|  2039.99|80667840-F962-4EE...|  2011-05-31|\n|       43659|                 8|         4911-403C-98|       3|      714|             1|    28.84|             0.00|    86.52|E9D54907-E7B7-496...|  2011-05-31|\n|       43659|                 9|         4911-403C-98|       1|      716|             1|    28.84|             0.00|    28.84|AA542630-BDCD-4CE...|  2011-05-31|\n|       43659|                10|         4911-403C-98|       6|      709|             1|     5.70|             0.00|    34.20|AC769034-3C2F-495...|  2011-05-31|\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AzureFunctionDateProcessing\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"SalesOrderID\", IntegerType(), True),\n",
    "    StructField(\"SalesOrderDetailID\", IntegerType(), True),\n",
    "    StructField(\"CarrierTrackingNumber\", StringType(), True),\n",
    "    StructField(\"OrderQty\", IntegerType(), True),\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"SpecialOfferID\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DecimalType(10, 2), True),\n",
    "    StructField(\"UnitPriceDiscount\", DecimalType(10, 2), True),\n",
    "    StructField(\"LineTotal\", DecimalType(20, 2), True),\n",
    "    StructField(\"rowguid\", StringType(), True),\n",
    "    StructField(\"ModifiedDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Load the data with the schema\n",
    "df = spark.read.format('csv').option('header', 'false').schema(schema).load(\"/FileStore/tables/Sales_SalesOrderDetail-2.csv\")\n",
    "\n",
    "# Show few rows to verify data loading\n",
    "df.show(10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64758deb-5b62-4a7f-88a3-2a4d874f46fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d33008-423f-44d1-a044-60a351b15ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+------------+\n|SalesOrderID|SalesOrderDetailID|CarrierTrackingNumber|OrderQty|ProductID|SpecialOfferID|UnitPrice|UnitPriceDiscount|LineTotal|             rowguid|ModifiedDate|\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+------------+\n|       43659|                 1|         4911-403C-98|       1|      776|             1|  2024.99|             0.00|  2024.99|B207C96D-D9E6-402...|  2011-05-31|\n|       43659|                 2|         4911-403C-98|       3|      777|             1|  2024.99|             0.00|  6074.98|7ABB600D-1E77-41B...|  2011-05-31|\n|       43659|                 3|         4911-403C-98|       1|      778|             1|  2024.99|             0.00|  2024.99|475CF8C6-49F6-486...|  2011-05-31|\n|       43659|                 4|         4911-403C-98|       1|      771|             1|  2039.99|             0.00|  2039.99|04C4DE91-5815-45D...|  2011-05-31|\n|       43659|                 5|         4911-403C-98|       1|      772|             1|  2039.99|             0.00|  2039.99|5A74C7D2-E641-438...|  2011-05-31|\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+------------+\nonly showing top 5 rows\n\nDataFrame[SalesOrderID: int, SalesOrderDetailID: int, CarrierTrackingNumber: string, OrderQty: int, ProductID: int, SpecialOfferID: int, UnitPrice: decimal(10,2), UnitPriceDiscount: decimal(10,2), LineTotal: decimal(20,2), rowguid: string, ModifiedDate: date, OrderYear: int, PreviousYearDate: date]\nDataFrame[SalesOrderID: int, SalesOrderDetailID: int, CarrierTrackingNumber: string, OrderQty: int, ProductID: int, SpecialOfferID: int, UnitPrice: decimal(10,2), UnitPriceDiscount: decimal(10,2), LineTotal: decimal(20,2), rowguid: string, ModifiedDate: date, OrderYear: int, PreviousYearDate: date]\nDataFrame[SalesOrderID: int, SalesOrderDetailID: int, CarrierTrackingNumber: string, OrderQty: int, ProductID: int, SpecialOfferID: int, UnitPrice: decimal(10,2), UnitPriceDiscount: decimal(10,2), LineTotal: decimal(20,2), rowguid: string, ModifiedDate: date, OrderYear: int, PreviousYearDate: date]\nDataFrame[SalesOrderID: int, SalesOrderDetailID: int, CarrierTrackingNumber: string, OrderQty: int, ProductID: int, SpecialOfferID: int, UnitPrice: decimal(10,2), UnitPriceDiscount: decimal(10,2), LineTotal: decimal(20,2), rowguid: string, ModifiedDate: date, OrderYear: int, PreviousYearDate: date]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesDataProcessing\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"SalesOrderID\", IntegerType(), True),\n",
    "    StructField(\"SalesOrderDetailID\", IntegerType(), True),\n",
    "    StructField(\"CarrierTrackingNumber\", StringType(), True),\n",
    "    StructField(\"OrderQty\", IntegerType(), True),\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"SpecialOfferID\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DecimalType(10, 2), True),\n",
    "    StructField(\"UnitPriceDiscount\", DecimalType(10, 2), True),\n",
    "    StructField(\"LineTotal\", DecimalType(20, 2), True),\n",
    "    StructField(\"rowguid\", StringType(), True),\n",
    "    StructField(\"ModifiedDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Load the data with the schema\n",
    "#df = spark.read.format('csv').option('header', 'false').schema(schema).load(\"/FileStore/tables/Sales_SalesOrderDetail.csv\")\n",
    "df = spark.read.format('csv').option('header', 'false').schema(schema).load(\"/FileStore/tables/Sales_SalesOrderDetail-2.csv\")\n",
    "\n",
    "# Show the first few rows to verify the data\n",
    "df.show(5)\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"SalesData\")\n",
    "\n",
    "# Extract distinct years\n",
    "years_df = spark.sql(\"SELECT DISTINCT YEAR(ModifiedDate) AS OrderYear FROM SalesData\")\n",
    "years = years_df.collect()\n",
    "years_list = [row[\"OrderYear\"] for row in years]\n",
    "\n",
    "for y in years_list:\n",
    "    # Define and execute the query\n",
    "    query = f\"\"\"\n",
    "    SELECT *, \n",
    "           YEAR(ModifiedDate) AS OrderYear, \n",
    "           DATE_SUB(ModifiedDate, 365) AS PreviousYearDate\n",
    "    FROM SalesData\n",
    "    WHERE YEAR(ModifiedDate) = {y}\n",
    "    \"\"\"\n",
    "    \n",
    "    df_year = spark.sql(query)\n",
    "    \n",
    "    print(df_year)\n",
    "    # Save the results to a Delta table\n",
    "    df_year.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"default.sales_data2_{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d19c0a-82b3-4e78-9411-016cc330f1e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n|       ModifiedDate|   ModifiedDate_UTC|   ModifiedDate_IST|\n+-------------------+-------------------+-------------------+\n|2011-05-31 00:00:00|2011-05-31 00:00:00|2011-05-31 05:30:00|\n|2011-05-31 00:00:00|2011-05-31 00:00:00|2011-05-31 05:30:00|\n|2011-05-31 00:00:00|2011-05-31 00:00:00|2011-05-31 05:30:00|\n|2011-05-31 00:00:00|2011-05-31 00:00:00|2011-05-31 05:30:00|\n|2011-05-31 00:00:00|2011-05-31 00:00:00|2011-05-31 05:30:00|\n|2011-05-31 00:00:00|2011-05-31 00:00:00|2011-05-31 05:30:00|\n+-------------------+-------------------+-------------------+\nonly showing top 6 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_utc_timestamp, from_utc_timestamp\n",
    "\n",
    "# Create Spark session (skip if already created)\n",
    "spark = SparkSession.builder.appName(\"TimezoneConversion\").getOrCreate()\n",
    "\n",
    "# Load the data into a DataFrame (assuming a CSV file in Databricks)\n",
    "#df = spark.read.option(\"header\", True).csv(\"/mnt/data/Sales.SalesOrderDetail.csv\")\n",
    "\n",
    "# Convert 'ModifiedDate' column to timestamp (adjust the column name if necessary)\n",
    "df = df.withColumn(\"ModifiedDate\", col(\"ModifiedDate\").cast(\"timestamp\"))\n",
    "\n",
    "# Convert to UTC\n",
    "df = df.withColumn(\"ModifiedDate_UTC\", to_utc_timestamp(col(\"ModifiedDate\"), \"UTC\"))\n",
    "\n",
    "# Convert to IST (Indian Standard Time)\n",
    "df = df.withColumn(\"ModifiedDate_IST\", from_utc_timestamp(col(\"ModifiedDate_UTC\"), \"Asia/Kolkata\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.select(\"ModifiedDate\", \"ModifiedDate_UTC\", \"ModifiedDate_IST\").show(6)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2430259850024287,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Last_year_2024-09-02 11:36:33",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

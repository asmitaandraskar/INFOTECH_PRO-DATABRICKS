{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "088dc085-a5da-4aea-b0d0-aae26a001c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Bronze Stage ETL Process Using PySpark and Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c070e3b0-0899-425a-91f7-09d81733966b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1: Load Data into DataFrames\n",
    "Bronze Stage ETL Process Using\n",
    "PySpark and Databricks\n",
    "Initialize a Spark session.\n",
    "Define a schema for the CSV data.\n",
    "Read data into a DataFrame with the specified schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2666852d-eed8-499b-87ca-8f4ce419bb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7689435-ddde-4b30-a2be-b8b4cbdacf4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession with the necessary option to handle S3\n",
    "spark = SparkSession.builder.appName(\"AWS S3 Integration and Data Processing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ed37c7-b03a-4c92-9ef1-84db2898343b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS S3 access keys securely\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\n",
    "\"AKIAZG4APFAQRRDBUO65\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\n",
    "\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef2a00d-5fe3-4c52-8c76-d5bc0a7524e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define S3 bucket and file paths\n",
    "bucket_name = \"iiht-ntt-24\"\n",
    "customer_file_path = f\"s3a://{bucket_name}/data.csv/Customer.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61d205b5-b721-4142-9478-274a49d156c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define the schema for the Customer CSV based on the initial definition provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469d054e-e241-493e-badd-b5adcd4a70c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "\n",
    "\n",
    "    StructField(\"CustomerID\",\n",
    "IntegerType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"CustomerName\",\n",
    "StringType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"BillToCustomerID\",\n",
    "StringType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"CustomerCategoryID\",\n",
    "StringType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"BuyingGroupID\",\n",
    "StringType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"PrimaryContactPersonID\",\n",
    "StringType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"PostalCityID\",\n",
    "StringType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"ValidFrom\",\n",
    "DateType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"ValidTo\",\n",
    "DateType(), True),\n",
    "\n",
    "\n",
    "    StructField(\"LineageKey\",\n",
    "IntegerType(), True)\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c6ee07-f394-4a30-b125-5339a8d01348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 4: Data Preparation\n",
    "Perform any necessary data cleaning or transformations required before joining the tables.Map raw data columns to the format required for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdba16e4-91fc-41ab-8623-eeed5aa46158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+------------------+-------------+----------------------+------------+----------+----------+----------+\n|CustomerID|CustomerName|BillToCustomerID|CustomerCategoryID|BuyingGroupID|PrimaryContactPersonID|PostalCityID| ValidFrom|   ValidTo|LineageKey|\n+----------+------------+----------------+------------------+-------------+----------------------+------------+----------+----------+----------+\n|      null|CustomerName|BillToCustomerID|CustomerCategoryID|BuyingGroupID|  PrimaryContactPer...|PostalCityID|      null|      null|      null|\n|         1|  Alice Corp|              10|               100|         1000|                   101|        9001|2023-01-01|2024-01-01|         9|\n|         2|     Bob LLC|              20|               200|         2000|                   202|        9002|2023-02-01|2024-02-01|         9|\n|         3| Charlie Inc|              30|               300|         3000|                   303|        9003|2023-03-01|2024-03-01|         9|\n|         4|   Delta Ltd|              40|               400|         4000|                   404|        9004|2023-04-01|2024-04-01|         9|\n+----------+------------+----------------+------------------+-------------+----------------------+------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Read the Customer CSV file into a DataFrame using the defined schema\n",
    "customer_df = spark.read.format(\"csv\").schema(customer_schema).load(\"/FileStore/tables/customer_data.csv\")\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b14805-c6e0-4081-b65d-897e637e6826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the data preparation step, Transform the DataFrame to match our target schema, renaming columns where necessary. \n",
    "The LineageKey column is set to a static value to help with data lineage and tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9969e66c-7c76-4b3e-9621-e2fc5af90918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+------------------+-------------+----------------------+------------+----------+----------+----------+\n|CustomerID|CustomerName|BillToCustomerID|CustomerCategoryID|BuyingGroupID|PrimaryContactPersonID|PostalCityID| ValidFrom|   ValidTo|LineageKey|\n+----------+------------+----------------+------------------+-------------+----------------------+------------+----------+----------+----------+\n|      null|CustomerName|BillToCustomerID|CustomerCategoryID|BuyingGroupID|  PrimaryContactPer...|PostalCityID|      null|      null|      null|\n|         1|  Alice Corp|              10|               100|         1000|                   101|        9001|2023-01-01|2024-01-01|         9|\n|         2|     Bob LLC|              20|               200|         2000|                   202|        9002|2023-02-01|2024-02-01|         9|\n|         3| Charlie Inc|              30|               300|         3000|                   303|        9003|2023-03-01|2024-03-01|         9|\n|         4|   Delta Ltd|              40|               400|         4000|                   404|        9004|2023-04-01|2024-04-01|         9|\n+----------+------------+----------------+------------------+-------------+----------------------+------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform DataFrame\n",
    "customer_df_transformed = customer_df.selectExpr(\n",
    "    \"CustomerID as CustomerKey\",\n",
    "    \"CustomerID as WWICustomerID\",\n",
    "    \"CustomerName as Customer\",\n",
    "    \"BillToCustomerID as BillToCustomer\",\n",
    "    \"CustomerCategoryID as Category\",\n",
    "    \"BuyingGroupID as BuyingGroup\",\n",
    "    \"PrimaryContactPersonID as PrimaryContact\",\n",
    "    \"PostalCityID as PostalCode\",\n",
    "    \"ValidFrom\", \"ValidTo\",\n",
    "    \"LineageKey\"\n",
    ").withColumn(\"LineageKey\", lit(9))\n",
    "\n",
    "customer_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deea7551-69ec-47d2-bf39-923db3d88b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 5: Save or Export Results\n",
    "Persist the prepared data into a table for use in subsequent ETL stages.\n",
    "save the prepared DataFrame to a table in the Spark SQL catalog named BronzeCustomer. The mode(\"overwrite\") option ensures we replace any existing data with our latest DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73d01e6-c9c6-4475-9a2f-daf5ed034449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table SilverCustomer1 created/overwritten successfully.\n"
     ]
    }
   ],
   "source": [
    "table_name = \"SilverCustomer1\"\n",
    "\n",
    "try:\n",
    "    # Check if table exists in the catalog\n",
    "    if table_name in [table.name for table in spark.catalog.listTables()]:\n",
    "        # Drop the table and delete its directory\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark._jsc.hadoopConfiguration()\n",
    "        ).delete(spark._jvm.org.apache.hadoop.fs.Path(\"dbfs:/user/hive/warehouse/silvercustomer1\"), True)\n",
    "\n",
    "    # Write the transformed DataFrame to the table\n",
    "    customer_df_transformed.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    print(f\"Table {table_name} created/overwritten successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create or replace table {table_name} due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5595e2b5-5420-4aa9-86bb-9c543dbcc147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+----------------+------------------+-------------+--------------------+------------+----------+----------+----------+\n|CustomerKey|WWICustomerID|    Customer|  BillToCustomer|          Category|  BuyingGroup|      PrimaryContact|  PostalCode| ValidFrom|   ValidTo|LineageKey|\n+-----------+-------------+------------+----------------+------------------+-------------+--------------------+------------+----------+----------+----------+\n|       null|         null|CustomerName|BillToCustomerID|CustomerCategoryID|BuyingGroupID|PrimaryContactPer...|PostalCityID|      null|      null|         9|\n|          1|            1|  Alice Corp|              10|               100|         1000|                 101|        9001|2023-01-01|2024-01-01|         9|\n|          2|            2|     Bob LLC|              20|               200|         2000|                 202|        9002|2023-02-01|2024-02-01|         9|\n|          3|            3| Charlie Inc|              30|               300|         3000|                 303|        9003|2023-03-01|2024-03-01|         9|\n|          4|            4|   Delta Ltd|              40|               400|         4000|                 404|        9004|2023-04-01|2024-04-01|         9|\n+-----------+-------------+------------+----------------+------------------+-------------+--------------------+------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1949d1-aa05-45e7-8bbb-4f57ff4ccc35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Silver Stage ETL Process Using PySpark and Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44549bdb-2b3d-4feb-b25a-869ceef280a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The Silver stage in an ETL process represents the refinement of extracted data. Data from the Bronze layer is cleansed, transformed, and enriched to create a reliable dataset for analysis and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0038d6-218a-43c5-888d-d5c87b10b069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Load Data into DataFrames\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Initialize SparkSession with necessary options to handle S3\n",
    "spark = SparkSession.builder.appName(\"AWS S3 Integration and Data Processing\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8e957b-0c40-42c2-88bb-30f54f9c79bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS S3 access keys securely (use environment variables or Databricks secrets in production)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAZG4APFAQRRDBUO65\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401f098e-86c7-4f8e-9d36-414f1774997b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define S3 bucket and file paths\n",
    "bucket_name = \"iiht-ntt-24\"\n",
    "sale_order_file_path = f\"s3a://{bucket_name}/data.csv/SaleOrder.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c785acd6-87de-4fe0-97b7-3dc01cabfd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c6b31dc-1dbe-49f3-97bf-c52d5cdbbdad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start by setting up the SparkSession, which is essential for any data processing in Spark. We then define the location and format of our source data (Sales data CSV file), load it into a DataFrame without schema inference (to speed up the process), and display it for a quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a9649f-1019-4b8e-9549-f0ffce76748d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+\n|OrderID|CustomerID|ProductID| OrderDate|Quantity|UnitPrice|Discount|TotalAmount| ShippingAddress|    Status|\n+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+\n|   1001|    2001.0|    101.0|2024-01-01|     5.0|     20.0|     0.1|       90.0|123 Street, City|   Shipped|\n|   1002|    2002.0|    102.0|2024-01-05|     3.0|     15.0|    null|       45.0|456 Avenue, City|Processing|\n|   1003|    2003.0|    103.0|2024-01-10|    null|     30.0|    0.05|       85.5|  789 Blvd, City|  Canceled|\n|   1004|    2004.0|    104.0|2024-01-12|     7.0|     25.0|    null|      175.0|123 Street, City|   Shipped|\n|   1005|    2005.0|     null|2024-01-15|     2.0|     40.0|     0.1|       null|456 Avenue, City|Processing|\n|   1006|      null|    106.0|2024-01-20|     4.0|     22.5|     0.2|       72.0|  789 Blvd, City|   Shipped|\n|   1007|    2007.0|    107.0|2024-01-25|    10.0|     18.0|    null|      180.0|123 Street, City|Processing|\n|   1008|    2008.0|    108.0|      null|     1.0|     50.0|    0.15|       42.5|            null|  Returned|\n+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# CSV options\n",
    "infer_schema = True  # Changed to boolean\n",
    "first_row_is_header = True  # Changed to boolean\n",
    "delimiter = \",\"  # This remains a string\n",
    "\n",
    "# Try block for processing and handling the Sales data\n",
    "try:\n",
    "    # Load the SaleOrder.csv file from S3 into a DataFrame\n",
    "    dfSales = spark.read.format(\"csv\") \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .option(\"header\", first_row_is_header) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .load(\"/FileStore/tables/sales_orders.csv\")\n",
    "\n",
    "    # Display the DataFrame to verify contents\n",
    "    dfSales.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data from S3 due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a06183e-48d7-43bd-aac2-f58da622381a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+\n|OrderID|CustomerID|ProductID| OrderDate|Quantity|UnitPrice|Discount|TotalAmount| ShippingAddress|    Status|\n+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+\n|   1001|    2001.0|    101.0|2024-01-01|     5.0|     20.0|     0.1|       90.0|123 Street, City|   Shipped|\n|   1002|    2002.0|    102.0|2024-01-05|     3.0|     15.0|    null|       45.0|456 Avenue, City|Processing|\n|   1003|    2003.0|    103.0|2024-01-10|    null|     30.0|    0.05|       85.5|  789 Blvd, City|  Canceled|\n|   1004|    2004.0|    104.0|2024-01-12|     7.0|     25.0|    null|      175.0|123 Street, City|   Shipped|\n|   1005|    2005.0|     null|2024-01-15|     2.0|     40.0|     0.1|       null|456 Avenue, City|Processing|\n|   1006|      null|    106.0|2024-01-20|     4.0|     22.5|     0.2|       72.0|  789 Blvd, City|   Shipped|\n|   1007|    2007.0|    107.0|2024-01-25|    10.0|     18.0|    null|      180.0|123 Street, City|Processing|\n|   1008|    2008.0|    108.0|      null|     1.0|     50.0|    0.15|       42.5|            null|  Returned|\n+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+\n\nTable SalesTable created successfully.\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n|CustomerKey|WWICustomerID|   Customer|BillToCustomer|Category|BuyingGroup|PrimaryContact|PostalCode| ValidFrom|   ValidTo|LineageKey|\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n|          1|            1| Alice Corp|            10|     100|       1000|           101|      9001|2023-01-01|2024-01-01|         9|\n|          2|            2|    Bob LLC|            20|     200|       2000|           202|      9002|2023-02-01|2024-02-01|         9|\n|          3|            3|Charlie Inc|            30|     300|       3000|           303|      9003|2023-03-01|2024-03-01|         9|\n|          4|            4|  Delta Ltd|            40|     400|       4000|           404|      9004|2023-04-01|2024-04-01|         9|\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"  # Set as string for options\n",
    "first_row_is_header = \"true\"  # Set as string for options\n",
    "delimiter = \",\"  # This remains a string\n",
    "\n",
    "# Step 1: Load CSV file into a DataFrame\n",
    "try:\n",
    "    # Load the SaleOrder.csv file from Databricks FileStore into a DataFrame\n",
    "    dfSales = spark.read.format(\"csv\") \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .option(\"header\", first_row_is_header) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .load(\"/FileStore/tables/sales_orders.csv\")\n",
    "\n",
    "    # Display the DataFrame to verify contents\n",
    "    dfSales.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data from CSV file due to: {e}\")\n",
    "\n",
    "# Step 2: Create or Replace SalesTable in the Database\n",
    "try:\n",
    "    # Handle the SalesTable creation or replacement\n",
    "    table_name = \"SalesTable\"\n",
    "    warehouse_location = spark.conf.get(\"spark.sql.warehouse.dir\", \"dbfs:/FileStore/tables\")  \n",
    "    table_path = f\"{warehouse_location}/{table_name.lower()}\"\n",
    "\n",
    "    # Check if the table path exists and remove it if so\n",
    "    try:\n",
    "        if any(dbutils.fs.ls(table_path)):  # Checks if path contains files\n",
    "            dbutils.fs.rm(table_path, recurse=True)\n",
    "    except Exception:\n",
    "        print(f\"Path {table_path} does not exist or couldn't be listed. Proceeding to create table.\")\n",
    "\n",
    "    # Drop the table if it exists\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "    # Write the DataFrame to a parquet table\n",
    "    dfSales.write.format(\"parquet\").mode(\"overwrite\").option(\"path\", table_path).saveAsTable(table_name)\n",
    "    print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing Sales data: {e}\")\n",
    "\n",
    "# Step 3: Delta Table Operations\n",
    "try:\n",
    "    # Load the permanent table created previously\n",
    "    permanent_table_df = spark.table(\"SilverCustomer1\")\n",
    "\n",
    "    # Filter rows where CustomerKey is not null\n",
    "    silver_customers_df = permanent_table_df.filter(col(\"CustomerKey\").isNotNull())\n",
    "\n",
    "    # Display the filtered DataFrame\n",
    "    silver_customers_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Delta table operations: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "914718ba-3a2a-4cc3-9a18-4e81b4356745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Retrieve and utilize existing permanent tables in the data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "327df0d5-8cb3-4803-8be1-fab5543ed7f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We extract the pre-existing \"SilverCustomer\" table from the data warehouse, filter it to exclude any records with null CustomerKey values, and create a temporary view for subsequent SQL operations or additional transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b366ed-8827-4ae9-be07-ca31a94b0b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table SalesTable created successfully.\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n|CustomerKey|WWICustomerID|   Customer|BillToCustomer|Category|BuyingGroup|PrimaryContact|PostalCode| ValidFrom|   ValidTo|LineageKey|\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n|          1|            1| Alice Corp|            10|     100|       1000|           101|      9001|2023-01-01|2024-01-01|         9|\n|          2|            2|    Bob LLC|            20|     200|       2000|           202|      9002|2023-02-01|2024-02-01|         9|\n|          3|            3|Charlie Inc|            30|     300|       3000|           303|      9003|2023-03-01|2024-03-01|         9|\n|          4|            4|  Delta Ltd|            40|     400|       4000|           404|      9004|2023-04-01|2024-04-01|         9|\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Join Tables\n",
    "try:\n",
    "    # Handle the SalesTable creation or replacement\n",
    "    table_name = \"SalesTable\"\n",
    "    warehouse_location = spark.conf.get(\"spark.sql.warehouse.dir\", \"dbfs:/FileStore/tables\")  \n",
    "    table_path = f\"{warehouse_location}/{table_name.lower()}\"\n",
    "\n",
    "    # Check if the directory exists and clear it\n",
    "    if dbutils.fs.ls(table_path):\n",
    "        dbutils.fs.rm(table_path, recurse=True)\n",
    "    \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "    dfSales.write.format(\"parquet\").mode(\"overwrite\").option(\"path\", table_path).saveAsTable(table_name)\n",
    "    print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing Sales data: {e}\")\n",
    "\n",
    "# Try block for handling Delta table operations\n",
    "try:\n",
    "    # Load the permanent table created previously\n",
    "    permanent_table_df = spark.table(\"SilverCustomer1\")\n",
    "\n",
    "    # Filter rows where CustomerKey is not null\n",
    "    silver_customers_df = permanent_table_df.filter(col(\"CustomerKey\").isNotNull())\n",
    "\n",
    "    # Display the filtered DataFrame\n",
    "    silver_customers_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Delta table operations: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780b13f7-1e45-4010-810e-f4d66390471c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we're taking our filtered customer data and storing it as a Delta table for fast, reliable, and versioned access to the dataset. Additionally, the raw sales data is stored in a permanent table named \"SalesTable\" for future analysis and reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6744ab56-e05b-4659-a35a-dab34fcfac16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table saved at /FileStore/tables/temp_silver_customers_table_delta\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n|CustomerKey|WWICustomerID|   Customer|BillToCustomer|Category|BuyingGroup|PrimaryContact|PostalCode| ValidFrom|   ValidTo|LineageKey|\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n|          1|            1| Alice Corp|            10|     100|       1000|           101|      9001|2023-01-01|2024-01-01|         9|\n|          2|            2|    Bob LLC|            20|     200|       2000|           202|      9002|2023-02-01|2024-02-01|         9|\n|          3|            3|Charlie Inc|            30|     300|       3000|           303|      9003|2023-03-01|2024-03-01|         9|\n|          4|            4|  Delta Ltd|            40|     400|       4000|           404|      9004|2023-04-01|2024-04-01|         9|\n+-----------+-------------+-----------+--------------+--------+-----------+--------------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Save or Export Results\n",
    "try:\n",
    "    # Create a temporary view or register DataFrame as a temporary table\n",
    "    silver_customers_df.createOrReplaceTempView(\"temp_silver_customers_table\")\n",
    "\n",
    "    # Define the Delta table path\n",
    "    delta_table_location = \"/FileStore/tables/temp_silver_customers_table_delta\"\n",
    "    dbutils.fs.rm(delta_table_location, True)  # Ensure the directory is empty\n",
    "\n",
    "    # Save the DataFrame as a Delta table\n",
    "    silver_customers_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_location)\n",
    "    print(f\"Delta table saved at {delta_table_location}\")\n",
    "\n",
    "    # Access and display the saved Delta table\n",
    "    delta_table_df = spark.read.format(\"delta\").load(delta_table_location)\n",
    "    delta_table_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the Delta table: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31263839-6817-4363-9cd1-38b7dac36efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Gold Stage ETL Process Using PySpark and Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f44ccc4-7df5-4b82-b6a9-88079c05c0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Initialize a Spark session.\n",
    "Define a complex schema for sales data.\n",
    "Create an empty DataFrame with a predefined schema to store sales orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0624c45-c941-420c-9628-fe43ca9f44c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderKey: string (nullable = false)\n |-- CustomerKey: integer (nullable = true)\n |-- CityKey: integer (nullable = true)\n |-- StockItemKey: integer (nullable = true)\n |-- OrderDateKey: date (nullable = true)\n |-- PickedDateKey: date (nullable = true)\n |-- SalespersonKey: integer (nullable = true)\n |-- PickerKey: integer (nullable = true)\n |-- WWIOrderID: integer (nullable = true)\n |-- WWIBackorderID: integer (nullable = true)\n |-- Description: string (nullable = true)\n |-- Package: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: decimal(18,2) (nullable = true)\n |-- TaxRate: decimal(18,3) (nullable = true)\n |-- TotalExcludingTax: decimal(18,2) (nullable = true)\n |-- TaxAmount: decimal(18,2) (nullable = true)\n |-- TotalIncludingTax: decimal(18,2) (nullable = true)\n |-- LineageKey: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "from pyspark.sql.functions import lit, udf\n",
    "import uuid\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Gold Table in Databricks PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema without spaces in field names\n",
    "schema = StructType([\n",
    "    StructField(\"OrderKey\", StringType(), nullable=False),\n",
    "    StructField(\"CustomerKey\", IntegerType(), nullable=True),\n",
    "    StructField(\"CityKey\", IntegerType(), nullable=True),\n",
    "    StructField(\"StockItemKey\", IntegerType(), nullable=True),\n",
    "    StructField(\"OrderDateKey\", DateType(), nullable=True),\n",
    "    StructField(\"PickedDateKey\", DateType(), nullable=True),\n",
    "    StructField(\"SalespersonKey\", IntegerType(), nullable=True),\n",
    "    StructField(\"PickerKey\", IntegerType(), nullable=True),\n",
    "    StructField(\"WWIOrderID\", IntegerType(), nullable=True),\n",
    "    StructField(\"WWIBackorderID\", IntegerType(), nullable=True),\n",
    "    StructField(\"Description\", StringType(), nullable=True),\n",
    "    StructField(\"Package\", StringType(), nullable=True),\n",
    "    StructField(\"Quantity\", IntegerType(), nullable=True),\n",
    "    StructField(\"UnitPrice\", DecimalType(18, 2), nullable=True),\n",
    "    StructField(\"TaxRate\", DecimalType(18, 3), nullable=True),\n",
    "    StructField(\"TotalExcludingTax\", DecimalType(18, 2), nullable=True),\n",
    "    StructField(\"TaxAmount\", DecimalType(18, 2), nullable=True),\n",
    "    StructField(\"TotalIncludingTax\", DecimalType(18, 2), nullable=True),\n",
    "    StructField(\"LineageKey\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Example of creating a DataFrame with the defined schema\n",
    "# You can create an empty DataFrame using this schema as follows:\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Show the DataFrame schema\n",
    "empty_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88691b03-c6e6-41d2-bf4c-d1d8ed84413a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Data Preparation\n",
    "Objectives\n",
    "Register the empty DataFrame as a temporary table for SQL operations.\n",
    "Load and prepare data from Delta and permanent tables for integration.\n",
    "Code and Actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440b88f1-e3ac-4931-ac69-38d60d2968e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+--------------------+\n|OrderID|CustomerID|ProductID| OrderDate|Quantity|UnitPrice|Discount|TotalAmount| ShippingAddress|    Status|            OrderKey|\n+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+--------------------+\n|   1001|    2001.0|    101.0|2024-01-01|     5.0|     20.0|     0.1|       90.0|123 Street, City|   Shipped|819e90b5-ecd4-4f7...|\n|   1002|    2002.0|    102.0|2024-01-05|     3.0|     15.0|    null|       45.0|456 Avenue, City|Processing|f4a9d633-e0c2-412...|\n|   1003|    2003.0|    103.0|2024-01-10|    null|     30.0|    0.05|       85.5|  789 Blvd, City|  Canceled|0ff7789d-3a12-4f5...|\n|   1004|    2004.0|    104.0|2024-01-12|     7.0|     25.0|    null|      175.0|123 Street, City|   Shipped|8340a63a-5cdd-40b...|\n|   1005|    2005.0|     null|2024-01-15|     2.0|     40.0|     0.1|       null|456 Avenue, City|Processing|1f5523e3-0faf-4be...|\n|   1006|      null|    106.0|2024-01-20|     4.0|     22.5|     0.2|       72.0|  789 Blvd, City|   Shipped|26e6fa60-4f63-4af...|\n|   1007|    2007.0|    107.0|2024-01-25|    10.0|     18.0|    null|      180.0|123 Street, City|Processing|de204e1e-acf8-444...|\n|   1008|    2008.0|    108.0|      null|     1.0|     50.0|    0.15|       42.5|            null|  Returned|9a14ce73-b340-4a6...|\n+-------+----------+---------+----------+--------+---------+--------+-----------+----------------+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import uuid\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Sales and Customer Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load customer_to_gold_df from Delta table\n",
    "    customer_to_gold_df = spark.read.format(\"delta\").load(\"/FileStore/tables/temp_silver_customers_table_delta\")\n",
    "\n",
    "    # Read data from the SalesTable\n",
    "    df_from_SalesOrder = spark.table(\"SalesTable\")\n",
    "\n",
    "    # Register the DataFrames as temporary views\n",
    "    df_from_SalesOrder.createOrReplaceTempView(\"SalesOrders\")\n",
    "    customer_to_gold_df.createOrReplaceTempView(\"CustomerToGold\")\n",
    "\n",
    "    # Generate GUIDs for Order Key using a UDF\n",
    "    generate_uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "    df_from_SalesOrder = df_from_SalesOrder.withColumn(\"OrderKey\", generate_uuid_udf())\n",
    "\n",
    "    # Show the updated DataFrame with the new OrderKey\n",
    "    df_from_SalesOrder.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1800c830-8fa9-450a-962c-3f4eaa838407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 3: Join Tables\n",
    "Objectives\n",
    "Use SQL to join sales and customer data.\n",
    "Generate unique identifiers for each order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75790d39-4b26-4bb4-b781-212ce8a7ae80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------------+------------+-------------+--------------+---------+----------+--------------+-----------+-------+--------+---------+-------+-----------------+---------+-----------------+----------+\n|OrderKey|CustomerKey|CityKey|StockItemKey|OrderDateKey|PickedDateKey|SalespersonKey|PickerKey|WWIOrderID|WWIBackorderID|Description|Package|Quantity|UnitPrice|TaxRate|TotalExcludingTax|TaxAmount|TotalIncludingTax|LineageKey|\n+--------+-----------+-------+------------+------------+-------------+--------------+---------+----------+--------------+-----------+-------+--------+---------+-------+-----------------+---------+-----------------+----------+\n|    1001|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1002|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1003|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1004|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1005|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1006|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1007|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n|    1008|       null|   null|        null|        null|         null|          null|     null|      null|          null|       null|   null|    null|     null|   null|             null|     null|             null|      null|\n+--------+-----------+-------+------------+------------+-------------+--------------+---------+----------+--------------+-----------+-------+--------+---------+-------+-----------------+---------+-----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    # Join the tables and select the necessary columns\n",
    "    joined_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            g.OrderID AS OrderKey,  -- Assuming OrderID is used as a unique OrderKey\n",
    "            c.CustomerKey, \n",
    "            NULL AS CityKey, \n",
    "            NULL AS StockItemKey, \n",
    "            NULL AS OrderDateKey, \n",
    "            NULL AS PickedDateKey, \n",
    "            NULL AS SalespersonKey, \n",
    "            NULL AS PickerKey, \n",
    "            NULL AS WWIOrderID, \n",
    "            NULL AS WWIBackorderID, \n",
    "            NULL AS Description, \n",
    "            NULL AS Package, \n",
    "            NULL AS Quantity, \n",
    "            NULL AS UnitPrice, \n",
    "            NULL AS TaxRate, \n",
    "            NULL AS TotalExcludingTax, \n",
    "            NULL AS TaxAmount, \n",
    "            NULL AS TotalIncludingTax, \n",
    "            NULL AS LineageKey\n",
    "        FROM SalesOrders g\n",
    "        LEFT JOIN CustomerToGold c ON g.CustomerID = c.WWICustomerID  -- Use correct fields for the join\n",
    "    \"\"\")\n",
    "\n",
    "    # Show the joined DataFrame to verify contents\n",
    "    joined_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the join operation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c047cff-8436-4f21-a2ac-af5edb0bb066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 4: Save or Export Results\n",
    "Objectives\n",
    "Union the joined data with the initially created empty DataFrame.\n",
    "Persist the final transformed data back into the GoldFactOrder table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0888217d-759e-492e-9a53-90b25a5f3808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderKey</th><th>CustomerKey</th><th>CityKey</th><th>StockItemKey</th><th>OrderDateKey</th><th>PickedDateKey</th><th>SalespersonKey</th><th>PickerKey</th><th>WWIOrderID</th><th>WWIBackorderID</th><th>Description</th><th>Package</th><th>Quantity</th><th>UnitPrice</th><th>TaxRate</th><th>TotalExcludingTax</th><th>TaxAmount</th><th>TotalIncludingTax</th><th>LineageKey</th></tr></thead><tbody><tr><td>1001</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1002</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1003</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1004</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1005</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1006</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1007</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1008</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1001,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1002,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1003,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1004,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1005,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1006,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1007,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         1008,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderKey",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CustomerKey",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CityKey",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "StockItemKey",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "OrderDateKey",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "PickedDateKey",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "SalespersonKey",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "PickerKey",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "WWIOrderID",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "WWIBackorderID",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "Description",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "Package",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "UnitPrice",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "TaxRate",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "TotalExcludingTax",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "TaxAmount",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "TotalIncludingTax",
         "type": "\"void\""
        },
        {
         "metadata": "{}",
         "name": "LineageKey",
         "type": "\"void\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    # Check if GoldFactOrder_df already exists\n",
    "    if 'GoldFactOrder_df' in locals():\n",
    "        # Union the existing GoldFactOrder_df with joined_df to keep all rows\n",
    "        final_df = GoldFactOrder_df.union(joined_df)\n",
    "    else:\n",
    "        # If GoldFactOrder_df doesn't exist, set it to joined_df\n",
    "        final_df = joined_df\n",
    "\n",
    "    # Replace the existing GoldFactOrder_df with the final DataFrame\n",
    "    GoldFactOrder_df = final_df\n",
    "\n",
    "    # Display the updated GoldFactOrder_df\n",
    "    display(GoldFactOrder_df)  # Use this if in Databricks, else use GoldFactOrder_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the Gold table creation process: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee57b9c-5fca-4f84-a033-93c48d9a68ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Highest Valued Customer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80c63cb-8f0a-4b81-a78f-aa7f7baa983a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, desc, coalesce, lit\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"High Value Customers Calculation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def calculate_high_value_customers(df: DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        # Ensure that the 'TotalIncludingTax' column is handled correctly if it contains null values\n",
    "        df = df.withColumn(\"TotalIncludingTax\", coalesce(col(\"TotalIncludingTax\"), lit(0)))\n",
    "\n",
    "        # Summing the total sales per customer and ordering them by the highest total\n",
    "        high_value_customers_df = df.groupBy(\"CustomerKey\") \\\n",
    "            .agg(sum(\"TotalIncludingTax\").alias(\"TotalSales\")) \\\n",
    "            .orderBy(desc(\"TotalSales\"))\n",
    "         \n",
    "         # Show the resulting DataFrame\n",
    "        print(\"High_Value_Customers:\")\n",
    "        \n",
    "        high_value_customers_df.show()\n",
    "        return high_value_customers_df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "029fddfc-d12e-4ac2-b100-dd35daa8c4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Track Frequently Visiting Customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea9ee60-601e-4e69-b67c-ede61973919e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n|CustomerKey|NumberOfOrders|\n+-----------+--------------+\n|       null|             8|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, countDistinct, desc\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Initialize SparkSession (if not already initialized)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Frequent Visitors Calculation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def calculate_frequent_visitors(df: DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        # Validate if necessary columns exist in the DataFrame to avoid runtime errors\n",
    "        required_columns = {\"CustomerKey\", \"OrderKey\"}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            raise ValueError(f\"DataFrame does not contain all required columns: {required_columns}\")\n",
    "\n",
    "        # Counting the distinct number of orders per customer and ordering the result\n",
    "        frequent_visitors_df = df.groupBy(\"CustomerKey\") \\\n",
    "            .agg(countDistinct(\"OrderKey\").alias(\"NumberOfOrders\")) \\\n",
    "            .orderBy(desc(\"NumberOfOrders\"))\n",
    "\n",
    "        return frequent_visitors_df\n",
    "\n",
    "    except AnalysisException as e:\n",
    "        print(f\"An error occurred during DataFrame operations: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# Example Usage:\n",
    "# Load data into GoldFactOrder_df (replace with your actual data source)\n",
    "# GoldFactOrder_df = spark.read.format(\"parquet\").load(\"/path/to/gold_fact_order_data\")\n",
    "\n",
    "# Calculate frequent visitors using the defined DataFrame\n",
    "frequent_visitors_df = calculate_frequent_visitors(GoldFactOrder_df)\n",
    "\n",
    "if frequent_visitors_df is not None:\n",
    "    frequent_visitors_df.show()\n",
    "else:\n",
    "    print(\"Failed to calculate frequent visitors due to an error.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "“Bronze Silver Gold Data extraction from CSV 2024-11-04 15:00:14",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3f4a6c-145b-49ad-b8e8-4f2d0c0d375e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+\n|start|end|sequence|\n+-----+---+--------+\n|    1|  5|       1|\n|    1|  5|       2|\n|    1|  5|       3|\n|    1|  5|       4|\n|    1|  5|       5|\n|   10| 15|      10|\n|   10| 15|      11|\n|   10| 15|      12|\n|   10| 15|      13|\n|   10| 15|      14|\n|   10| 15|      15|\n+-----+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sequence, explode, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Sequence and Explode\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a start and end column\n",
    "data = [(1, 5), (10, 15)]\n",
    "df = spark.createDataFrame(data, [\"start\", \"end\"])\n",
    "\n",
    "# Generate a sequence for each row and explode it\n",
    "df = df.withColumn(\"sequence\", explode(sequence(col(\"start\"), col(\"end\"))))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a691c6-6e5c-4c06-b549-188e7afe86b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+----------+\n|department|employee|salary|row_number|\n+----------+--------+------+----------+\n|        HR|   Alice|  7000|         1|\n|        HR|     Bob|  5500|         2|\n|     Sales|    John|  5000|         1|\n|     Sales|     Doe|  4500|         2|\n+----------+--------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Sales\", \"John\", 5000),\n",
    "        (\"Sales\", \"Doe\", 4500),\n",
    "        (\"HR\", \"Alice\", 7000),\n",
    "        (\"HR\", \"Bob\", 5500)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"department\", \"employee\", \"salary\"])\n",
    "\n",
    "# Define window specification\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply window function\n",
    "df = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4119e26-2fdd-4782-810a-9a503985d013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+----+\n|start|end|sequence|rank|\n+-----+---+--------+----+\n|    1|  3|       1|   1|\n|    1|  3|       2|   2|\n|    1|  3|       3|   3|\n|   10| 12|      10|   4|\n|   10| 12|      11|   5|\n|   10| 12|      12|   6|\n+-----+---+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 3), (10, 12)], [\"start\", \"end\"])\n",
    "\n",
    "# Create sequence and explode\n",
    "df = df.withColumn(\"sequence\", explode(sequence(col(\"start\"), col(\"end\"))))\n",
    "\n",
    "# Define window specification\n",
    "windowSpec = Window.orderBy(\"sequence\")\n",
    "\n",
    "# Apply rank function over the window\n",
    "df = df.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL seq and window rank in sql and pyspark2024-08-30 10:02:20",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
